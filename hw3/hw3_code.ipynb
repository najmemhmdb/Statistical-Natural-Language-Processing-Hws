{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3_99131009.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCnWYc9yZOXQ"
      },
      "source": [
        "try:\n",
        "  %numpy_version  1.20.3\n",
        "except:\n",
        "   pass\n",
        "!pip install bert-embedding\n",
        "from bert_embedding import BertEmbedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XluvztfqECom"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-o05bbnTr2m"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "import gensim\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.stem import PorterStemmer\n",
        "from gensim.models import Word2Vec,TfidfModel, LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvHe8obLfQYl"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snErqxFhvDTm"
      },
      "source": [
        "# load all files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WbLcd6sa5rt"
      },
      "source": [
        "def load_dict():\n",
        "    train_data = {}\n",
        "    test_data = {}\n",
        "    dictionary = {}\n",
        "    tree = ET.parse('drive/MyDrive/NLP/hw3/dictionary.xml')\n",
        "    root = tree.getroot()\n",
        "    # ambiguose words\n",
        "    for element in root:\n",
        "            word_pos = element.attrib['item']\n",
        "            train_data[element.attrib['item'][:-2]] = {\"pos\": element.attrib['item'][-1]}\n",
        "            test_data[element.attrib['item'][:-2]] = {\"pos\": element.attrib['item'][-1]}\n",
        "            dictionary[element.attrib['item'][:-2]] = {\"pos\": element.attrib['item'][-1]}\n",
        "\n",
        "    # various senses for each word\n",
        "    for word in train_data.keys():\n",
        "        for sense in root.findall(\"./lexelt[@item='{}']/sense\".format(word + \".\" + train_data[word][\"pos\"])):\n",
        "            train_data[word][sense.attrib['id']] = []\n",
        "            test_data[word][sense.attrib['id']] = []\n",
        "            dictionary[word][sense.attrib['id']] = sense.attrib\n",
        "    return dictionary, train_data, test_data\n",
        "\n",
        "\n",
        "def preprocessing(text):\n",
        "    punctuation = ['<','>',',','.','&','!','@','#','$','%','^','*','(',')','-','_','+','=',';','\\'','/','\\\\','{','}','[',']','?',':','\"']\n",
        "    stopwords_list = stopwords.words('english')\n",
        "    text = text.lower()\n",
        "    text_num = re.sub('[0-9]+', '', text)\n",
        "    words = word_tokenize(text_num)\n",
        "    filtered_words = [word for word in words if word not in punctuation]\n",
        "    filtered_words = [word for word in filtered_words if word not in stop_words]\n",
        "    return filtered_words\n",
        "\n",
        "\n",
        "def load_data(dataset, datapath):\n",
        "    train_data = pd.read_csv(datapath)\n",
        "    max_sequence_len = 0\n",
        "    all = []\n",
        "    for index, word_data in train_data.iterrows():\n",
        "            context = word_data[\"context\"]\n",
        "            before_text = context[0:context.index(\"<head>\")]\n",
        "            after_text = context[context.index(\"</head>\") + 7:]\n",
        "            before = 9\n",
        "            after = 10\n",
        "            context_b = preprocessing(before_text)\n",
        "            context_a = preprocessing(after_text)\n",
        "            if len(context_b) < 9:\n",
        "                before = len(context_b)\n",
        "            if len(context_a) < 10:\n",
        "                after = len(context_a)\n",
        "            context = context_b[-before:] + [word_data['word']]\n",
        "            context += context_a[:after]\n",
        "            dataset[word_data['word']][word_data[\"sense_id\"]].append(context)\n",
        "            all.append(context)\n",
        "    return dataset, all\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJlxD6QymsOo"
      },
      "source": [
        "fill train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owFvMfkpiMAD"
      },
      "source": [
        "    def train_xy(dictionary,train):\n",
        "        train_X = {}\n",
        "        data_train, train_all = load_data(train, \"drive/MyDrive/NLP/hw3/train.csv\")\n",
        "        for word in data_train.keys():\n",
        "            X = []\n",
        "            Y = []\n",
        "            y = list(dictionary[word].keys())\n",
        "            for sense in data_train[word].keys():\n",
        "                if sense != \"pos\":\n",
        "                    for vec in data_train[word][sense]:\n",
        "                        X.append(vec)\n",
        "                        Y.append(y.index(sense))\n",
        "            train_X[word] = {\"x\": X, \"y\": Y}\n",
        "        return train_X\n",
        "\n",
        "    def test_xy(dictionary,test):\n",
        "        test_X = {}\n",
        "        data_test, test_all = load_data(test, \"drive/MyDrive/NLP/hw3/test.csv\")\n",
        "        for word in data_test.keys():\n",
        "            X = []\n",
        "            Y = []\n",
        "            ys = list(dictionary[word].keys())\n",
        "            for sense in data_test[word].keys():\n",
        "                if sense != \"pos\":\n",
        "                    for vec in data_test[word][sense]:\n",
        "                        X.append(vec)\n",
        "                        Y.append(ys.index(sense))\n",
        "            test_X[word] = {\"x\": X, \"y\": Y}\n",
        "      return test_X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W4pnpEPpjka"
      },
      "source": [
        "tf_idf weighting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxbWoASbpmvW"
      },
      "source": [
        "    def tfidf_weigth(mode, words_array, tfidf, words):\n",
        "        s = 0\n",
        "        if mode == 0:\n",
        "            result = np.zeros(300)\n",
        "        else:\n",
        "            result = np.zeros(768)\n",
        "        tf_idf_list = tfidf[words]\n",
        "        i = 0\n",
        "        for idf in tf_idf_list:\n",
        "            for i, tfidf in idf:\n",
        "                try:\n",
        "                    result += tfidf * np.array(words_array[i])\n",
        "                except ValueError:\n",
        "                    print(words_array[i].shape, tfidf, result.shape)\n",
        "                s += tfidf\n",
        "        result /= s\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2Vjvp84pbtV"
      },
      "source": [
        "word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkqPkCB-nJnP"
      },
      "source": [
        "def word2vec_representation(dataset, docs, mode,dict):\n",
        "        data = {}\n",
        "        dictionary = Dictionary(docs)\n",
        "        BoW_corpus = [dictionary.doc2bow(text) for text in docs]\n",
        "        for word in dataset.keys():\n",
        "            labels = list(dict[word].keys())\n",
        "            embeddings = []\n",
        "            new_labels = []\n",
        "            for sense in dataset[word].keys():\n",
        "                if sense != \"pos\":\n",
        "                    for context in dataset[word][sense]:\n",
        "                        index = context.index(word)\n",
        "                        h = index - 3\n",
        "                        t = index + 4\n",
        "                        if index - 3 < 0:\n",
        "                            h = 0\n",
        "                        if index + 4 > len(context):\n",
        "                            t = len(context)\n",
        "                        bow_words = [dictionary.doc2bow(doc, allow_update=True) for doc in [context[h:t]]]\n",
        "                        if mode == 1:\n",
        "                            word_vec = {}\n",
        "                            for id, freq in bow_words[0]:\n",
        "                                try:\n",
        "                                    word_vec[id] = model[dictionary[id]]\n",
        "                                except IndexError:\n",
        "                                    pass\n",
        "                                except KeyError:\n",
        "                                    try:\n",
        "                                        word_vec[id] = model[dictionary[id] + \"e\"]\n",
        "                                    except KeyError:\n",
        "                                        word_vec[id] = np.zeros(300)\n",
        "                            embeddings.append(tfidf_weigth(0, word_vec,\n",
        "                                                                TfidfModel(BoW_corpus, smartirs='ntc'),\n",
        "                                                                bow_words))\n",
        "                        if mode == 2:\n",
        "                            bow_words = [dictionary.doc2bow(doc, allow_update=True) for doc in [context]]\n",
        "                            word_vec = {}\n",
        "                            for k, freq in bow_words[0]:\n",
        "                                try:\n",
        "                                    word_vec[k] = model[dictionary[k]]\n",
        "                                except IndexError:\n",
        "                                    pass\n",
        "                                except KeyError:\n",
        "                                    try:\n",
        "                                        word_vec[k] = model[dictionary[k] + \"e\"]\n",
        "                                    except KeyError:\n",
        "                                        word_vec[k] = np.zeros(300)\n",
        "                            embeddings.append(\n",
        "                                tfidf_weigth(0, word_vec, TfidfModel(BoW_corpus, smartirs='ntc'), bow_words))\n",
        "                        new_labels.append(labels.index(sense))\n",
        "            data[word] = {\"x\": embeddings, \"y\": new_labels}\n",
        "        return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgDEdmKopUoZ"
      },
      "source": [
        "Bert embedding and dimension reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5YLxDxEpO_T"
      },
      "source": [
        "    def dimension_reduction(vectors,train_X,test_X):\n",
        "        pca = PCA(n_components=300)\n",
        "        reducted_vectors = pca.fit_transform(vectors)\n",
        "        s = 0\n",
        "        for word in train_X.keys():\n",
        "            for sense in train_X[word].keys():\n",
        "                if sense != \"pos\":\n",
        "                    e = s + len(train_X[word][sense])\n",
        "                    train_X[word][sense] = reducted_vectors[s:e]\n",
        "                    s = e\n",
        "        for word in test_X.keys():\n",
        "            for sense in test_X[word].keys():\n",
        "                if sense != \"pos\":\n",
        "                    e = s + len(test_X[word][sense])\n",
        "                    test_X[word][sense] = reducted_vectors[s:e]\n",
        "                    s = e\n",
        "        return train_X,test_X\n",
        "\n",
        "    def bert_representation(data_set, docs, mode,arr_embedding_vectors):\n",
        "        bert_embedding = BertEmbedding(max_seq_length=40)\n",
        "        dictionary = Dictionary(docs)\n",
        "        if mode == 0:\n",
        "            for word in data_set.keys():\n",
        "                for sense in data_set[word].keys():\n",
        "                    if sense != \"pos\":\n",
        "                        context = data_set[word][sense]\n",
        "                        for word_c in context:\n",
        "                            str = \"\"\n",
        "                            for txt in word_c:\n",
        "                                str += txt + \" \"\n",
        "                            result = bert_embedding([str.strip(\" \")])\n",
        "                            index = word_c.index(word)\n",
        "                            arr_embedding_vectors.append(np.ndarray.flatten(np.array(result[0][1][index])))\n",
        "        if mode == 3:\n",
        "            for word in data_set.keys():\n",
        "                for sense in data_set[word].keys():\n",
        "                    if sense != \"pos\":\n",
        "                        context = data_set[word][sense]\n",
        "                        for word_c in context:\n",
        "                            str = \"[cls]\"\n",
        "                            for txt in word_c:\n",
        "                                str += txt + \" \"\n",
        "                            result = bert_embedding([str.strip(\" \")])\n",
        "                            index = word_c.index(word)\n",
        "                            if result[0][0][index] != word_c[index]:\n",
        "                                print(result[0][0], word_c)\n",
        "                            arr_embedding_vectors.append(np.ndarray.flatten(np.array(result[0][1][0])))\n",
        "        # Embedding words in a window\n",
        "        else:\n",
        "            for word in data_set.keys():\n",
        "                for sense in data_set[word].keys():\n",
        "                    print(word)\n",
        "                    if sense != \"pos\":\n",
        "                        context = data_set[word][sense]\n",
        "                        for word_c in context:\n",
        "                            str = \" \"\n",
        "                            for txt in word_c:\n",
        "                                str += txt + \" \"\n",
        "                            result = bert_embedding([str.strip()])\n",
        "                            if mode == 1:\n",
        "                                index = word_c.index(word)\n",
        "                                head = index - 3\n",
        "                                tail = index + 4\n",
        "                                if index - 3 < 0:\n",
        "                                    head = 0\n",
        "                                if index + 4 > len(word_c):\n",
        "                                    tail = len(word_c)\n",
        "                                bow_words = [dictionary.doc2bow(doc, allow_update=True) for doc in [word_c[head:tail]]]\n",
        "                                vector_dict = {}\n",
        "                                for id, freq in bow_words[0]:\n",
        "                                    vector = np.ndarray.flatten(np.array(result[0][1][word_c.index(dictionary[id])]))\n",
        "                                    vector_dict[id] = vector[:768]\n",
        "                                arr_embedding_vectors.append(self.tfidf_weigth(1, vector_dict,\n",
        "                                                                                    TfidfModel(BoW_corpus,\n",
        "                                                                                             smartirs='ntc'),\n",
        "                                                                                    bow_words))\n",
        "                            if mode == 2:\n",
        "                                bow_words = [dictionary.doc2bow(doc, allow_update=True) for doc in [word_c]]\n",
        "                                vector_dict = {}\n",
        "                                for id, freq in bow_words[0]:\n",
        "                                    try:\n",
        "                                        vector = np.ndarray.flatten(np.array(result[0][1][word_c.index(dictionary[id])]))\n",
        "                                        vector_dict[id] = vector[:768]\n",
        "                                    except IndexError:\n",
        "                                        vector = np.ndarray.flatten(np.array(result[0][1][0]))\n",
        "                                        vector_dict[id] = vector[:768]\n",
        "                                arr_embedding_vectors.append(self.tfidf_weigth(1, vector_dict,\n",
        "                                                                                    TfidfModel(BoW_corpus,\n",
        "                                                                                             smartirs='ntc'),\n",
        "                                                                                    bow_words))\n",
        "        return arr_embedding_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un9JijrasBYX"
      },
      "source": [
        "main block for creating necessary files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-rlWOdIkm-a"
      },
      "source": [
        "dict, train, test = load_dict()\n",
        "data_train, train_all = load_data(train, \"drive/MyDrive/NLP/hw3/train.csv\")\n",
        "data_test, test_all = load_data(test, \"drive/MyDrive/NLP/hw3/test.csv\")\n",
        "arr_output = []\n",
        "arr = bert_representation(data_train, train_all, 2,arr_output)\n",
        "arr = bert_representation(data_test, test_all, 2,arr)\n",
        "dimension_reduction(arr,train,test)\n",
        "train = train_xy(dict,train)\n",
        "test = test_xy(dict,test)\n",
        "## save bert_embedding vectors \n",
        "for word in train.keys():\n",
        "   pd.DataFrame(train[word]['x']).to_csv(\"drive/MyDrive/NLP/hw3/train_bert0/\"+word+\".csv\")\n",
        "   pd.DataFrame(train[word]['y']).to_csv(\"drive/MyDrive/NLP/hw3/train_bert0/\"+word+\"_label.csv\")\n",
        "for word in test.keys():\n",
        "   pd.DataFrame(test[word]['x']).to_csv(\"drive/MyDrive/NLP/hw3/test_bert0/\"+word+\".csv\")\n",
        "   pd.DataFrame(test[word]['y']).to_csv(\"drive/MyDrive/NLP/hw3/test_bert0/\"+word+\"_label.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuROHE3gg_s2"
      },
      "source": [
        "def fetch_data(path1,path2):\n",
        "  for word in dict_w.keys():\n",
        "    train_data = pd.read_csv(path1+\"/\"+word+\".csv\").to_numpy()\n",
        "    train_label = pd.read_csv(path1+\"/\"+word+\"_label.csv\").to_numpy()[:,1]\n",
        "    test_data = pd.read_csv(path2+\"/\"+word+\".csv\").to_numpy()\n",
        "    test_label = pd.read_csv(path2+\"/\"+word+\"_label.csv\").to_numpy()[:,1]\n",
        "    train[word] = {\"x\":train_data,\"y\":train_label}\n",
        "    test[word] = {\"x\":test_data,\"y\":test_label}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDnMlkc8HDe6"
      },
      "source": [
        "# Part 3(a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnFd2WweMo7S"
      },
      "source": [
        "def train_classifier(mode):\n",
        "  for word in dict_w.keys():\n",
        "    if mode == 0:\n",
        "      lgst_model = LogisticRegression(max_iter=20000,C=0.08)     \n",
        "      lgst_model.fit(train[word]['x'],train[word]['y'])\n",
        "      rf_model = RandomForestClassifier(n_estimators=150,max_depth=100)\n",
        "      rf_model.fit(train[word]['x'],train[word]['y'])\n",
        "      rf_classifier[word] = rf_model\n",
        "      lr_classifier[word] = lgst_model\n",
        "    if mode == 1:\n",
        "      rf_model = RandomForestClassifier(n_estimators=150,max_depth=100)\n",
        "      rf_model.fit(train[word]['x'],train[word]['y'])\n",
        "      rf_classifier1[word] = rf_model\n",
        "    if mode == 2:\n",
        "      rf_model = RandomForestClassifier(n_estimators=150,max_depth=100)\n",
        "      rf_model.fit(train1[word]['x'],train1[word]['y'])\n",
        "      rf_classifier2[word] = rf_model\n",
        "    if mode == 3:\n",
        "      rf_model = RandomForestClassifier(n_estimators=150,max_depth=100)\n",
        "      rf_model.fit(train2[word]['x'],train2[word]['y'])\n",
        "      rf_classifier3[word] = rf_model\n",
        "\n",
        "\n",
        "def classifier(classifiers):\n",
        "    POS = {}\n",
        "    measures = []\n",
        "    for word in classifiers.keys():\n",
        "            pred_y = classifiers[word].predict(test[word]['x'])\n",
        "            if word == 'paper':\n",
        "              print(test[word]['y'], pred_y)\n",
        "            accuracy = accuracy_score(test[word]['y'], pred_y)\n",
        "            f_score = f1_score(test[word]['y'], pred_y, average='micro')\n",
        "            measures.append([accuracy, f_score])\n",
        "            if word == 'paper':\n",
        "              print(measures[-1])\n",
        "            try:\n",
        "               POS[dict_w[word]['pos']][\"acc\"] += accuracy\n",
        "               POS[dict_w[word]['pos']][\"fm\"] += f_score\n",
        "               POS[dict_w[word]['pos']][\"c\"] += 1\n",
        "            except KeyError:\n",
        "              POS[dict_w[word]['pos']] ={\"acc\":accuracy,\n",
        "                                    \"fm\":f_score,\n",
        "                                    \"c\":1}\n",
        "    for p in POS.keys():\n",
        "        POS[p][\"acc\"] /= POS[p][\"c\"]\n",
        "        POS[p][\"fm\"] /= POS[p][\"c\"]\n",
        "    print(POS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8BEkqgpG0Xg"
      },
      "source": [
        "# part 3(b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTUKucTHWTu7"
      },
      "source": [
        "def enssemble(classifier1,classifier2,classifier3):\n",
        "    POS = {}\n",
        "    measures = []\n",
        "    for word in classifier1.keys(): \n",
        "            pred_y1 = classifier1[word].predict(test[word]['x'])\n",
        "            pred_y2 = classifier2[word].predict(test1[word]['x'])\n",
        "            pred_y3 = classifier3[word].predict(test2[word]['x'])\n",
        "            pred_y = predict(pred_y1, pred_y2, pred_y3)\n",
        "            accuracy = accuracy_score(test[word]['y'], pred_y)\n",
        "            f_score = f1_score(test[word]['y'], pred_y, average='micro')\n",
        "            measures.append([float(\"{0:.4f}\".format(accuracy)), float(\"{0:.4f}\".format(f_score))])\n",
        "            try:\n",
        "               POS[dict_w[word]['pos']][\"acc\"] += accuracy\n",
        "               POS[dict_w[word]['pos']][\"fm\"] += f_score\n",
        "               POS[dict_w[word]['pos']][\"c\"] += 1\n",
        "            except KeyError:\n",
        "              POS[dict_w[word]['pos']] ={\"acc\":accuracy,\n",
        "                                    \"fm\":f_score,\n",
        "                                    \"c\":1}\n",
        "    for p in POS.keys():\n",
        "        POS[p][\"acc\"] /= POS[p][\"c\"]\n",
        "        POS[p][\"fm\"] /= POS[p][\"c\"]\n",
        "    print(POS)\n",
        "\n",
        "def predict(first,second,third):\n",
        "  y = []\n",
        "  for l1,l2,l3 in zip(first,second,third):\n",
        "       y.append(np.bincount([l1,l2,l3]).argmax())\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEZzAfLyGfAV"
      },
      "source": [
        "# part 3(c)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lesq48PiYO_T"
      },
      "source": [
        "def transformed_data(pred1,pred2,pred3):\n",
        "  pred_y = []\n",
        "  for y1,y2,y3 in zip(pred1,pred2,pred3):\n",
        "       pred_y.append([y1,y2,y3])\n",
        "  return pred_y\n",
        "\n",
        "def enssemble_v2(classifier1,classifier2,classifier3):\n",
        "    POS = {}\n",
        "    measures = []\n",
        "    for word in classifier1.keys(): \n",
        "            pred_y1 = classifier1[word].predict(test[word]['x'])\n",
        "            pred_y2 = classifier2[word].predict(test1[word]['x'])\n",
        "            pred_y3 = classifier3[word].predict(test2[word]['x'])\n",
        "            test_x = transformed_data(pred_y1, pred_y2, pred_y3)\n",
        "            pred_y1 = classifier1[word].predict(train[word]['x'])\n",
        "            pred_y2 = classifier2[word].predict(train1[word]['x'])\n",
        "            pred_y3 = classifier3[word].predict(train2[word]['x'])\n",
        "            train_x = transformed_data(pred_y1, pred_y2, pred_y3)  \n",
        "            rf_model = RandomForestClassifier(n_estimators=150,max_depth=100)\n",
        "            rf_model.fit(train_x, train[word]['y'])\n",
        "            predict_label =rf_model.predict(test_x) \n",
        "            accuracy = accuracy_score(test[word]['y'],predict_label)\n",
        "            f_score = f1_score(test[word]['y'], predict_label, average='micro')\n",
        "            measures.append([float(\"{0:.4f}\".format(accuracy)), float(\"{0:.4f}\".format(f_score))])\n",
        "            try:\n",
        "               POS[dict_w[word]['pos']][\"acc\"] += accuracy\n",
        "               POS[dict_w[word]['pos']][\"fm\"] += f_score\n",
        "               POS[dict_w[word]['pos']][\"c\"] += 1\n",
        "            except KeyError:\n",
        "              POS[dict_w[word]['pos']] ={\"acc\":accuracy,\n",
        "                                    \"fm\":f_score,\n",
        "                                    \"c\":1}\n",
        "    for p in POS.keys():\n",
        "        POS[p][\"acc\"] /= POS[p][\"c\"]\n",
        "        POS[p][\"fm\"] /= POS[p][\"c\"]\n",
        "\n",
        "    print(POS)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UH6ECgay7zf"
      },
      "source": [
        "main block for part 3(a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1k1G3GMfuYP"
      },
      "source": [
        "dict_w, _,_ = load_dict() \n",
        "data_train, train_all = load_data(train, \"drive/MyDrive/NLP/hw3/train.csv\")\n",
        "data_test, test_all = load_data(test, \"drive/MyDrive/NLP/hw3/test.csv\")\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "     'drive/MyDrive/NLP/hw3/GoogleNews-vectors-negative300.bin.gz',binary=True)\n",
        "## bert_vectors without context + logistic regression & random forest classifier\n",
        "train = {}\n",
        "test = {}\n",
        "fetch_data(\"drive/MyDrive/NLP/hw3/train_bert0\",\"drive/MyDrive/NLP/hw3/test_bert0\")\n",
        "lr_classifier = {}\n",
        "rf_classifier = {}\n",
        "train_classifier(0)\n",
        "classifier(lr_classifier) \n",
        "classifier(rf_classifier) \n",
        "#3 bert_vectors with 3 length window + logistic regression & random forest classifier\n",
        "train = {}\n",
        "test = {}\n",
        "fetch_data(\"drive/MyDrive/NLP/hw3/train_bert1\",\"drive/MyDrive/NLP/hw3/test_bert1\")\n",
        "lr_classifier = {}\n",
        "rf_classifier = {}\n",
        "train_classifier(0)\n",
        "classifier(lr_classifier) \n",
        "classifier(rf_classifier) \n",
        "# bert_vectors with 9 length window + logistic regression & random forest classifier\n",
        "train = {}\n",
        "test = {}\n",
        "fetch_data(\"drive/MyDrive/NLP/hw3/train_bert2\",\"drive/MyDrive/NLP/hw3/test_bert2\")\n",
        "lr_classifier = {}\n",
        "rf_classifier = {}\n",
        "train_classifier(0)\n",
        "classifier(lr_classifier) \n",
        "classifier(rf_classifier) \n",
        "# word2vec with 3 length window + logistic regression & random forest classifier\n",
        "train = word2vec_representation(data_train,train_all,1,dict_w)\n",
        "test = word2vec_representation(data_test,test_all,1,dict_w)\n",
        "lr_classifier = {}\n",
        "rf_classifier = {}\n",
        "train_classifier(0)\n",
        "classifier(lr_classifier) \n",
        "classifier(rf_classifier) \n",
        "# word2vec with 9 length window + logistic regression & random forest classifier\n",
        "train = word2vec_representation(data_train,train_all,2,dict_w)\n",
        "test = word2vec_representation(data_test,test_all,2,dict_w)\n",
        "lr_classifier = {}\n",
        "rf_classifier = {}\n",
        "train_classifier(0)\n",
        "classifier(lr_classifier) \n",
        "classifier(rf_classifier) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiEXDoFly_zf"
      },
      "source": [
        "main block for part 3(b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJnSOphDJLoQ"
      },
      "source": [
        "rf_classifier1 = {}\n",
        "rf_classifier2 = {}\n",
        "rf_classifier3 = {}\n",
        "train = {}\n",
        "test = {}\n",
        "fetch_data(\"drive/MyDrive/NLP/hw3/train_bert0\",\"drive/MyDrive/NLP/hw3/test_bert0\")\n",
        "train1 = word2vec_representation(data_train,wsd.train_all,1,dict_w)\n",
        "test1 = word2vec_representation(data_test,wsd.test_all,1,dict_w)\n",
        "train2 = word2vec_representation(data_train,wsd.train_all,2,dict_w)\n",
        "test2 = word2vec_representation(data_test,wsd.test_all,2,dict_w)\n",
        "train_classifier(1)\n",
        "train_classifier(2)\n",
        "train_classifier(3)\n",
        "enssemble(rf_classifier1,rf_classifier2,rf_classifier3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyZhUJ3mzFG7"
      },
      "source": [
        "main block for part 3(c)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bX4Q3Nq2DWu"
      },
      "source": [
        "enssemble_v2(rf_classifier1,rf_classifier2,rf_classifier3)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}